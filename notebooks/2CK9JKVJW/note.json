{
  "paragraphs": [
    {
      "text": "%md\nWelcome to Spark Datahack course\n![Spark](https://spark.apache.org/images/spark-logo-trademark.png)![Datahack](http://www.datahack.es/wp-content/uploads/2015/10/LogoWhite.png)\n\n",
      "user": "anonymous",
      "dateUpdated": "Jun 7, 2017 7:47:09 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eWelcome to Spark Datahack course\u003cbr/\u003e\u003cimg src\u003d\"https://spark.apache.org/images/spark-logo-trademark.png\" alt\u003d\"Spark\" /\u003e\u003cimg src\u003d\"http://www.datahack.es/wp-content/uploads/2015/10/LogoWhite.png\" alt\u003d\"Datahack\" /\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1496773400966_426901212",
      "id": "20170606-182320_455215110",
      "dateCreated": "Jun 6, 2017 6:23:20 PM",
      "dateStarted": "Jun 7, 2017 7:47:09 AM",
      "dateFinished": "Jun 7, 2017 7:47:09 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//old way of reading csv\r\nval bankText \u003d sc.textFile(\"/usr/zeppelin/data/bank/bank-full.csv\")\r\ncase class Bank(age:Integer, job:String, marital : String, education : String, balance : Integer)\r\n\r\n// split each line, filter out header (starts with \"age\"), and map it into Bank case class  \r\nval bank \u003d bankText.map(s\u003d\u003es.split(\";\")).filter(s\u003d\u003es(0)!\u003d\"\\\"age\\\"\").map(\r\n    s\u003d\u003eBank(s(0).toInt, \r\n            s(1).replaceAll(\"\\\"\", \"\"),\r\n            s(2).replaceAll(\"\\\"\", \"\"),\r\n            s(3).replaceAll(\"\\\"\", \"\"),\r\n            s(5).replaceAll(\"\\\"\", \"\").toInt\r\n        )\r\n)\r\n\r\n// convert to DataFrame and create temporal table\r\nbank.toDF().registerTempTable(\"bank\")\r\n\r\n",
      "user": "anonymous",
      "dateUpdated": "Jun 7, 2017 7:56:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbankText: org.apache.spark.rdd.RDD[String] \u003d /usr/zeppelin/data/bank/bank-full.csv MapPartitionsRDD[730] at textFile at \u003cconsole\u003e:42\n\ndefined class Bank\n\nbank: org.apache.spark.rdd.RDD[Bank] \u003d MapPartitionsRDD[733] at map at \u003cconsole\u003e:45\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1496696452676_1189682229",
      "id": "20170605-210052_1887255984",
      "dateCreated": "Jun 5, 2017 9:00:52 PM",
      "dateStarted": "Jun 7, 2017 7:56:50 AM",
      "dateFinished": "Jun 7, 2017 7:56:57 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \nAge distribution from bank",
      "user": "anonymous",
      "dateUpdated": "Jun 7, 2017 7:47:10 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eAge distribution from bank\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1496773887064_1662250020",
      "id": "20170606-183127_1953703662",
      "dateCreated": "Jun 6, 2017 6:31:27 PM",
      "dateStarted": "Jun 7, 2017 7:47:10 AM",
      "dateFinished": "Jun 7, 2017 7:47:10 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql select age, count(1) from bank where age \u003c 30 group by age order by age\n",
      "user": "anonymous",
      "dateUpdated": "Jun 7, 2017 7:47:10 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "multiBarChart",
              "height": 300.0,
              "optionOpen": false
            },
            "helium": {}
          }
        },
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Table or view not found: bank; line 1 pos 26\nset zeppelin.spark.sql.stacktrace \u003d true to see full stacktrace"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1496774000194_397534413",
      "id": "20170606-183320_2108168495",
      "dateCreated": "Jun 6, 2017 6:33:20 PM",
      "dateStarted": "Jun 7, 2017 7:47:10 AM",
      "dateFinished": "Jun 7, 2017 7:47:14 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nAge distribution with certain marital status\n\n",
      "user": "anonymous",
      "dateUpdated": "Jun 7, 2017 7:47:10 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1496774040806_940677267",
      "id": "20170606-183400_176056686",
      "dateCreated": "Jun 6, 2017 6:34:00 PM",
      "dateStarted": "Jun 7, 2017 7:47:11 AM",
      "dateFinished": "Jun 7, 2017 7:47:11 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\nselect age, count(1) from bank where marital\u003d\"${marital\u003dsingle,single|divorced|married}\" group by age order by age\n",
      "user": "anonymous",
      "dateUpdated": "Jun 7, 2017 7:47:11 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "multiBarChart",
              "height": 300.0,
              "optionOpen": false
            },
            "helium": {}
          }
        },
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sql",
        "runOnSelectionChange": true
      },
      "settings": {
        "params": {
          "marital": "single"
        },
        "forms": {
          "marital": {
            "name": "marital",
            "defaultValue": "single",
            "options": [
              {
                "value": "single"
              },
              {
                "value": "divorced"
              },
              {
                "value": "married"
              }
            ],
            "hidden": false
          }
        }
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Table or view not found: bank; line 1 pos 26\nset zeppelin.spark.sql.stacktrace \u003d true to see full stacktrace"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1496773677409_-236022787",
      "id": "20170606-182757_1365789518",
      "dateCreated": "Jun 6, 2017 6:27:57 PM",
      "dateStarted": "Jun 7, 2017 7:47:12 AM",
      "dateFinished": "Jun 7, 2017 7:47:14 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//spark 2.1 way of reading csv files\r\nval df \u003d spark.read\r\n        .format(\"csv\")\r\n        .option(\"header\", \"true\") //reading the headers\r\n        .option(\"delimiter\",\";\") //set delimiter \r\n        .option(\"mode\", \"DROPMALFORMED\")\r\n        .csv(\"/usr/zeppelin/data/bank/bank-full.csv\")\r\ndf.show()",
      "user": "anonymous",
      "dateUpdated": "Jun 7, 2017 7:47:11 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njava.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:38)\norg.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:33)\norg.apache.zeppelin.spark.SparkInterpreter.createSparkSession(SparkInterpreter.java:367)\norg.apache.zeppelin.spark.SparkInterpreter.getSparkSession(SparkInterpreter.java:233)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:826)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:483)\norg.apache.zeppelin.scheduler.Job.run(Job.java:175)\norg.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\njava.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\njava.util.concurrent.FutureTask.run(FutureTask.java:266)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n\n  at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n  at org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2126)\n  at org.apache.spark.SparkContext.defaultMinPartitions(SparkContext.scala:2135)\n  at org.apache.spark.SparkContext.textFile$default$2(SparkContext.scala:819)\n  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.readText(CSVFileFormat.scala:215)\n  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.baseRdd(CSVFileFormat.scala:180)\n  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:59)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:184)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:184)\n  at scala.Option.orElse(Option.scala:289)\n  at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$getOrInferFileFormatSchema(DataSource.scala:183)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:387)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:415)\n  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:352)\n  ... 58 elided\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1496773080624_-1626764179",
      "id": "20170606-181800_315918021",
      "dateCreated": "Jun 6, 2017 6:18:00 PM",
      "dateStarted": "Jun 7, 2017 7:47:14 AM",
      "dateFinished": "Jun 7, 2017 7:47:16 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "dateUpdated": "Jun 7, 2017 7:47:11 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1496773728205_683804564",
      "id": "20170606-182848_1978297950",
      "dateCreated": "Jun 6, 2017 6:28:48 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Spark Intro",
  "id": "2CK9JKVJW",
  "angularObjects": {
    "2CHD3N6XP:shared_process": [],
    "2CHXHJFU6:shared_process": [],
    "2CK5F94SS:shared_process": [],
    "2CJERHQTQ:shared_process": [],
    "2CM9UPU1U:shared_process": [],
    "2CJUP3PRB:shared_process": [],
    "2CHDQBZ8C:shared_process": [],
    "2CK1MPJQK:shared_process": [],
    "2CJBA4UNV:shared_process": [],
    "2CJUCKWCX:shared_process": []
  },
  "config": {},
  "info": {}
}